# 분류 평가
```python
def get_clf_eval(y_test, pred, pred_proba=None, multi_class='raise', average='binary'):
    from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred, average=average)
    recall = recall_score(y_test, pred, average=average)
    f1 = f1_score(y_test, pred, average=average)
    if pred_proba is not None:
        # multi_class ``'ovr'`` or ``'ovo'``
        # average micro, macro, weighted, samples
        roc_auc = roc_auc_score(y_test, pred_proba, multi_class=multi_class, average=average if average in ['micro', 'macro', 'weighted', 'samples'] else None)
        print("오차 행렬")
        print(confusion)
        print("정확도:{:.4f}, 정밀도:{:.4f}, 재현율:{:.4f}, F1:{:.4f}, AUC:{:.4f}".format(accuracy,precision,recall,f1,roc_auc))
    else:
        print("오차 행렬")
        print(confusion)
        print("정확도:{:.4f}, 정밀도:{:.4f}, 재현율:{:.4f}, F1:{:.4f}".format(accuracy,precision,recall,f1))


def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):
    model.fit(ftr_train, tgt_train)
    pred = model.predict(ftr_test)
    pred_proba = model.predict_proba(ftr_test)[:,1]
    get_clf_eval(tgt_test, pred, pred_proba)

# get_clf_eval(y_test, pred)
# get_clf_eval(y_test, pred, pred_proba=clf.predict_proba(x_test)[:,1])    
```

# classification_report 분석결과
```python
from sklearn.metrics import classification_report
report = classification_report(y_test,pred)
print(report)
"""
              precision    recall  f1-score   support

           0       0.52      0.31      0.39        90
           1       0.75      0.88      0.81       210

    accuracy                           0.71       300
   macro avg       0.63      0.59      0.60       300
weighted avg       0.68      0.71      0.68       300
"""
```

# ROC 커브 AUC 시각화
```python
import matplotlib.pyplot as plt
from sklearn.metrics import plot_roc_curve ,RocCurveDisplay
fig, ax = plt.subplots(1,2, figsize=(12,6))

plot_roc_curve(clf, x_test, y_test, ax = ax[0]) # 시각화 비교
RocCurveDisplay.from_estimator(clf, x_test, y_test, ax = ax[1]) # 시각화 비교

plt.tight_layout()
plt.show()
```

# 로직스택 회귀 그리기
```python
proba = pd.DataFrame(logR.predict_proba(train_x))
cs = logR.decision_function(train_x)
df = pd.concat([proba, pd.DataFrame(cs)], axis=1)
df.columns = ['Not A','A','decision_function']
df.sort_values(['decision_function'], inplace=True)
df.reset_index(inplace=True, drop=True)

def lr_plot(decision_function, not_a, a):
    import matplotlib.pyplot as plt
    plt.figure(figsize=(15, 5))
    plt.axhline(y=0.5, linestyle='--', color='black', linewidth=1)
    plt.axvline(x=0, linestyle='--', color='black', linewidth=1)
    
    plt.plot(decision_function, not_a, 'g--', label='Not A')
    plt.plot(decision_function, not_a, 'g^')
    plt.plot(decision_function, a, 'b--', label='A')
    plt.plot(decision_function, a, 'b*')
    plt.xlabel("")
    plt.ylabel("")
    plt.legend(loc='upper left')

    
lr_plot(df['decision_function'], df['Not A'], df['A'])
```


# SVC plot
```python
def svc_plot(X_train_sc, y_train, C_list):
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.svm import LinearSVC

    for i, C in enumerate(C_list):
        clf = LinearSVC(C=C, loss='hinge', random_state=42, max_iter=100000).fit(X_train_sc, y_train)

        # decision function으로 서포트 벡터 얻기
        decision_function = clf.decision_function(X_train_sc)
        support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]
        support_vectors = X_train_sc[support_vector_indices]
        plt.subplot(len(C_list)//2+1, 2 if len(C_list) >= 2 else len(C_list), i+1)
        plt.scatter(X_train_sc[:,0], X_train_sc[:,1], c=y_train, s=30, cmap=plt.cm.Paired)
        ax = plt.gca()
        ylim = ax.get_ylim()
        xx, yy = np.meshgrid(
            np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)
        )
        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        plt.contour(xx,yy,Z,colors='k',levels=[-1,0,1],linestyles=['--','-','--'])
        plt.scatter(support_vectors[:,0], support_vectors[:,1],s=100,linewidth=1, facecolors='none',edgecolors='k')
        plt.title("C=" + str(C))

    plt.tight_layout()
    
    
plt.figure(figsize=(12,8))
svc_plot(X_train_sc, y_train, [1, 500])
plt.show()
```

# 결정트리 계열 feature_importance 
```python
def feature_importance(clf):
    return pd.DataFrame({
        'feature_nm': clf.feature_names_in_,
        'importances' : clf.feature_importances_
    })

def feature_importance_plot_sns(clf, top_n=20):
    import matplotlib.pyplot as plt
    import seaborn as sns
    df = feature_importance(clf)
    ftr_top = df.sort_values('importances' ,ascending=False).head(top_n)
    plt.title(f"Feature importances Top {top_n}")
    sns.barplot(x=ftr_top['importances'], y=ftr_top['feature_nm'])


def feature_importance_plot(clf, top_n=20):
    import matplotlib.pyplot as plt
    import numpy as np
    df = feature_importance(clf)
    ftr_top = df.sort_values('importances' ,ascending=False).head(top_n)
    ftr_top = ftr_top.sort_values('importances' ,ascending=True)
    plt.title(f"Feature importances Top {top_n}")

    plt.barh(range(len(ftr_top['feature_nm'])), ftr_top['importances'], align="center")
    plt.yticks(np.arange(len(ftr_top['feature_nm'])),ftr_top['feature_nm'])
    plt.xlabel("feature importances")
    plt.ylabel("feature")

# plt.figure(figsize=(8,6))
feature_importance_plot(clf, top_n=10)
plt.show()
```

# 결정트리의 분류 의사결정 시각화
```python
from sklearn.tree import plot_tree
from matplotlib import pyplot as plt

fig = plt.figure(figsize=(15, 8))
plot_tree(dt_clf, class_names=iris.target_names
          , feature_names=iris.feature_names, impurity=True, filled=True,fontsize=10)
plt.show()
```

# Classifier의 Decision Boundary를 시각화 하는 함수
```python
def visualize_boundary(model, X, y):
    import matplotlib.pyplot as plt
    import numpy as np
    fig,ax = plt.subplots()
    
    # 학습 데이타 scatter plot으로 나타내기
    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',
               clim=(y.min(), y.max()), zorder=3)
    ax.axis('tight')
    ax.axis('off')
    xlim_start , xlim_end = ax.get_xlim()
    ylim_start , ylim_end = ax.get_ylim()
    
    # 호출 파라미터로 들어온 training 데이타로 model 학습 . 
    model.fit(X, y)
    # meshgrid 형태인 모든 좌표값으로 예측 수행. 
    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    
    # contourf() 를 이용하여 class boundary 를 visualization 수행. 
    n_classes = len(np.unique(y))
    contours = ax.contourf(xx, yy, Z, alpha=0.3,
                           levels=np.arange(n_classes + 1) - 0.5,
                           cmap='rainbow',
                           zorder=1)

# from sklearn.tree import DecisionTreeClassifier
# dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)
# visualize_boundary(dt_clf, X_features, y_labels)
```